# ğŸ“Š Level 1 (Beginner) Data Science Tasks : Web Scraping, Cleaning, and EDA
# Organization : CODVEDA 
# Role : Data Science Intern


## ğŸš€ Project Overview
This project demonstrates a complete data science workflow from data collection to exploratory analysis. It consists of three main tasks that cover the essential steps of any data analysis project.

## âœ… Tasks Completed

### 1. ğŸ•·ï¸ Data Collection and Web Scraping
- ğŸ” Identified target website and inspected its HTML structure
- âš™ï¸ Implemented web scraping using BeautifulSoup and requests libraries
- ğŸ”„ Handled pagination and dynamic content challenges
- ğŸ’¾ Stored scraped data in structured CSV format
- **ğŸ› ï¸ Tools Used**: Python, BeautifulSoup, requests, pandas

### 2. ğŸ§¹ Data Cleaning and Preprocessing
- ğŸ§© Handled missing data through imputation and removal strategies
- ğŸ“Š Detected and removed outliers from the dataset
- ğŸ”¢ Performed categorical variable encoding (one-hot and label encoding)
- âš–ï¸ Normalized numerical data for consistent scaling
- **ğŸ› ï¸ Tools Used**: Python, pandas, scikit-learn

### 3. ğŸ” Exploratory Data Analysis (EDA)
- ğŸ“ˆ Calculated comprehensive summary statistics
- ğŸ“Š Created visualizations including histograms, scatter plots, and box plots
- ğŸ”— Analyzed feature correlations using correlation matrices
- ğŸ“ Documented key insights and observations
- **ğŸ› ï¸ Tools Used**: Python, pandas, matplotlib, seaborn


## ğŸ”® Future Work
- ğŸ¤– Potential to extend the analysis with machine learning models
- ğŸŒ Expand data collection to additional sources
- âš¡ Implement automated data pipeline

## ğŸ“‹ Requirements
- ğŸ Python 3.x
- ğŸ“š Libraries: pandas, BeautifulSoup, requests, scikit-learn, matplotlib, seaborn
