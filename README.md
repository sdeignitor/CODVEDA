# 📊 Level 1 (Beginner) Data Science Tasks : Web Scraping, Cleaning, and EDA
# Organization : CODVEDA 
# Role : Data Science Intern


## 🚀 Project Overview
This project demonstrates a complete data science workflow from data collection to exploratory analysis. It consists of three main tasks that cover the essential steps of any data analysis project.

## ✅ Tasks Completed

### 1. 🕷️ Data Collection and Web Scraping
- 🔍 Identified target website and inspected its HTML structure
- ⚙️ Implemented web scraping using BeautifulSoup and requests libraries
- 🔄 Handled pagination and dynamic content challenges
- 💾 Stored scraped data in structured CSV format
- **🛠️ Tools Used**: Python, BeautifulSoup, requests, pandas

### 2. 🧹 Data Cleaning and Preprocessing
- 🧩 Handled missing data through imputation and removal strategies
- 📊 Detected and removed outliers from the dataset
- 🔢 Performed categorical variable encoding (one-hot and label encoding)
- ⚖️ Normalized numerical data for consistent scaling
- **🛠️ Tools Used**: Python, pandas, scikit-learn

### 3. 🔍 Exploratory Data Analysis (EDA)
- 📈 Calculated comprehensive summary statistics
- 📊 Created visualizations including histograms, scatter plots, and box plots
- 🔗 Analyzed feature correlations using correlation matrices
- 📝 Documented key insights and observations
- **🛠️ Tools Used**: Python, pandas, matplotlib, seaborn


## 🔮 Future Work
- 🤖 Potential to extend the analysis with machine learning models
- 🌐 Expand data collection to additional sources
- ⚡ Implement automated data pipeline

## 📋 Requirements
- 🐍 Python 3.x
- 📚 Libraries: pandas, BeautifulSoup, requests, scikit-learn, matplotlib, seaborn
